---
title: 简单的优化方法
date: 2017-09-04 11:52:23
tags: [数学,优化算法]
---


# 梯度下降法

梯度下降法的原理是**利用关于最优解的信息**，不断逼近最优解。
梯度下降方法有效的前提条件是，**梯度方向指向最优解方向**。

当优化目标有大量局部极值的时候，绝大多数解空间位置的梯度方向不再指向最优解。

## 梯度下降法的组成
- 函数1：待求的函数
- 函数2：待求的函数的导数
- 变量1：当前找到的变量
- 变量2：梯度grad，对大多数函数来说就是函数的负导数
- 变量3：步长step，沿着梯度下降方向进行的步长

## 例子1：参数为一维
[参考资料](https://zhuanlan.zhihu.com/p/21486804)

对于函数$f(x) = x^2 - 2x + 1$，其函数图像为：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/162627059.png?imageslim)

``` python
def f(x):
    return x * x - 2 * x + 1

def g(x): # f(x)的导数
    return 2 * x - 2

def gd(x_start, step, g):   # gd代表了Gradient Descent
    x = x_start
    for i in range(40):
        grad = g(x)
        x -= grad * step
        print ('[ Epoch {0} ] grad = {1}, x = {2}'.format(i, grad, x))
        if abs(grad) < 1e-6:#若当前梯度已经很小了，可以认为找到了极值
            break
    return x

gd(5,0.1,g)#当前变量 步长 梯度函数
```
梯度下降的起点是5，步长为0.1，使用的梯度函数是$g(x) = 2x-2 $。


## 梯度
实际上梯度指的是**当前变量处的梯度**，对于这一点，它的梯度方向是这个方向，沿着这个梯度方向走，函数值是会下降的。
但是如果一步迈得太大，就会跳出函数值下降的范围，反而会使函数值越变越大。所以**小的步长会让优化问题收敛**，**大的步长会让优化问题发散**。
收敛和发散的阈值就是当其**梯度函数为0**的时候。

## 例子2：参数为二维
[参考资料](http://blog.csdn.net/xiaoch1222/article/details/52847521)
### 问题的提出
假设有这样的样本（样本值来自$ y = 3x1 + 4x2$）:

| x1  | x2  | y   |
| --- | --- | --- |
| 1   | 4   | 19  |
| 2   | 5   | 26  |
| 5   | 1   | 19  |
| 4   | 2   | 29  |
x1和x2是样本值，y是预测值，我们需要一条直线来拟合上面的数据，待拟合的函数入下图：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210153943.png?imageslim)
我们的目的就是求出θ1和θ2的值，让h(θ)尽量逼近目标值。这是一个线性回归的问题，利用最小二乘法和梯度下降法可以求出这两个参数。


### BGD(Batch Gradient Descent)
批量梯度下降法，每次迭代都需要把所有样本都送入。这样的好处是每次迭代都顾及了全部的样本，做的是全局最优化。

确定损失函数![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210117704.png?imageslim)

我们的目标是让损失函数，所以根据梯度下降法，用J(θ)对θ求偏导：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210311064.png?imageslim)

由于是要**最小化损失函数**，所以参数θ按其负梯度方向来更新：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210415207.png?imageslim)


``` python
import random
#用y = Θ1*x1 + Θ2*x2来拟合下面的输入和输出
#input1  1   2   5   4
#input2  4   5   1   2
#output  19  26  19  20
input_x = [[1,4], [2,5], [5,1], [4,2]]  #输入
y = [19,26,19,20]   #输出
theta = [1,1]       #θ参数初始化
loss = 10           #loss先定义一个数，为了进入循环迭代
step_size = 0.01    #步长
eps =0.0001         #精度要求
max_iters = 10000   #最大迭代次数
error =0            #损失值
iter_count = 0      #当前迭代次数

err1=[0,0,0,0]      #求Θ1梯度的中间变量1
err2=[0,0,0,0]      #求Θ2梯度的中间变量2

while( loss > eps and iter_count < max_iters):   #迭代条件
    loss = 0
    err1sum = 0
    err2sum = 0

    for i in range (4):     #每次迭代所有的样本都进行训练
        pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]  #预测值
        err1[i]=(pred_y-y[i])*input_x[i][0]#
        err1sum=err1sum+err1[i]
        err2[i]=(pred_y-y[i])*input_x[i][1]#
        err2sum=err2sum+err2[i]

    theta[0] = theta[0] - step_size * err1sum/4  #对应5式
    theta[1] = theta[1] - step_size * err2sum/4  #对应5式

    for i in range (4):
        pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]   #预测值
        error = (1/(2*4))*(pred_y - y[i])**2  #损失值
        loss = loss + error  #总损失值

    iter_count += 1

    print ("iters_count", iter_count)

print ('theta: ',theta )
print ('final loss: ', loss)
print ('iters: ', iter_count)
```


### SGD(Stochastic Gradient Descent)
随机梯度下降法，每次迭代使用一组样本。
因为BGD算法训练速度过慢，所以提出了SGD，普通的BGD算法是每次迭代把所有样本都过一遍，每训练一组样本就把梯度更新一次。

而SGD算法是从样本中抽出一组，训练后按梯度更新一次，然后继续抽取，继续更新，在样本量很大的情况下，可以不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型。

### MBGD(Mini-Batch Gradient Descent)
小批量梯度下降，每次迭代使用b组样本。
SGD由于单个样本可能会带来很多噪声，使得SGD并不是每次迭代的时候都向着整体最优化方向进行，因此在刚开始的时候可能会收敛很快，但是训练一段时间之后可能会变慢。

所以在此基础上又提出了小批量梯度下降，每次从样本中随机抽取一小批进行训练。
