---
title: 常间的几种最优化方法
date: 2017-09-04 11:52:23
tags: [数学,优化方法]
---

因为在毕设中需要对一些数据进行优化算法，所以这里简单记录一下常见的几种最优化方法。

最优化方法是一种数学方法，是研究在给定约束之下如何寻找某些值，以使某一指标达到最优的一些学科的总称。


# 梯度下降法(Gradient Descent)
梯度下降法是最早最简单，也是最常用的优化方法。**当目标函数是凸函数的时候，梯度下降法的解是全局最优解**。但是在一般的情况下，其解不保证是全局最优解，而且其速度也不是最快的。
梯度下降法的优化思想是**用当前位置负梯度方向作为搜索方向**，因为该方向为当前位置的最快下降方向。梯度下降法越接近目标值，步长越小，前进越慢。

梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次迭代。

## 梯度下降法相关的概念
1. 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

2. 特征（feature）：指的是样本中输入部分，比如样本（x0,y0）,（x1,y1）,则样本特征为x，样本输出为y。

3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)。比如对于样本（xi,yi）(i=1,2,...n),可以采用拟合函数如下： $hθ(x) = θ0+θ1x$。

4. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,...n),采用线性回归，损失函数为：
$J(θ0,θ1)=∑i=1m(hθ(xi)−yi)^2$
其中$xi$表示样本特征x的第i个元素，$yi$表示样本输出y的第i个元素，$hθ(xi)$为假设函数。   

## 梯度下降的详细算法
梯度下降法的算法由代数法和矩阵法两种表示。[参考资料](http://www.cnblogs.com/pinard/p/5970503.html)

### 代数方式表示

### 矩阵方式表示


## BGD(Batch Gradient Descent)
批量梯度下降法，每次迭代都需要把所有样本都送入。这样的好处是每次迭代都顾及了全部的样本，做的是全局最优化。

## SGD(Stochastic Gradient Descent)
随机梯度下降法，每次迭代使用一组样本。
因为BGD算法训练速度过慢，所以提出了SGD。
普通的BGD算法是每次迭代把所有样本都过一遍，每训练一组样本就把梯度更新一次。而SGD算法是从样本中抽出一组，训练后按梯度更新一次，然后继续抽取，继续更新，在样本量很大的情况下，可以不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型。

## MBGD(Mini-Batch Gradient Descent)
小批量梯度下降，每次迭代使用b组样本。
SGD由于单个样本可能会带来很多噪声，使得SGD并不是每次迭代的时候都向着整体最优化方向进行，因此在刚开始的时候可能会收敛很快，但是训练一段时间之后可能会变慢。

所以在此基础上又提出了小批量梯度下降，每次从样本中随机抽取一小批进行训练。


## 例子1：参数为一维
[参考资料](https://zhuanlan.zhihu.com/p/21486804)

对于函数$f(x) = x^2 - 2x + 1$，其函数图像为：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/162627059.png?imageslim)

``` python
def f(x):
    return x * x - 2 * x + 1

def g(x): # f(x)的导数
    return 2 * x - 2

def gd(x_start, step, g):   # gd代表了Gradient Descent
    x = x_start
    for i in range(40):
        grad = g(x)
        x -= grad * step
        print ('[ Epoch {0} ] grad = {1}, x = {2}'.format(i, grad, x))
        if abs(grad) < 1e-6:#若当前梯度已经很小了，可以认为找到了极值
            break
    return x

gd(5,0.1,g)#当前变量 步长 梯度函数
```
梯度下降的起点是5，步长为0.1，使用的梯度函数是$g(x) = 2x-2 $。



## 例子2：参数为二维
[参考资料](http://blog.csdn.net/xiaoch1222/article/details/52847521)
### 问题的提出
假设有这样的样本（样本值来自$ y = 3x1 + 4x2$）:

| x1  | x2  | y   |
| --- | --- | --- |
| 1   | 4   | 19  |
| 2   | 5   | 26  |
| 5   | 1   | 19  |
| 4   | 2   | 29  |
x1和x2是样本值，y是预测值，我们需要一条直线来拟合上面的数据，待拟合的函数如下图：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210153943.png?imageslim)
我们的目的就是求出θ1和θ2的值，让h(θ)尽量逼近目标值。这是一个线性回归的问题，利用梯度下降法可以求出这两个参数。

确定损失函数 ： ![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210117704.png?imageslim)

我们的目标是最小化损失函数，所以根据梯度下降法，用J(θ)对θ求偏导：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210311064.png?imageslim)

由于是要**最小化损失函数**，所以参数θ按其负梯度方向来更新：
![mark](http://o9z9uibed.bkt.clouddn.com/image/20170904/210415207.png?imageslim)


``` python
import random
#用y = Θ1*x1 + Θ2*x2来拟合下面的输入和输出
#input1  1   2   5   4
#input2  4   5   1   2
#output  19  26  19  20
input_x = [[1,4], [2,5], [5,1], [4,2]]  #输入
y = [19,26,19,20]   #输出
theta = [1,1]       #θ参数初始化
loss = 10           #loss先定义一个数，为了进入循环迭代
step_size = 0.01    #步长
eps =0.0001         #精度要求
max_iters = 10000   #最大迭代次数
error =0            #损失值
iter_count = 0      #当前迭代次数

err1=[0,0,0,0]      #求Θ1梯度的中间变量1
err2=[0,0,0,0]      #求Θ2梯度的中间变量2

while( loss > eps and iter_count < max_iters):   #迭代条件
    loss = 0
    err1sum = 0
    err2sum = 0

    for i in range (4):     #每次迭代所有的样本都进行训练
        pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]  #预测值
        err1[i]=(pred_y-y[i])*input_x[i][0]
        err1sum=err1sum+err1[i]
        err2[i]=(pred_y-y[i])*input_x[i][1]
        err2sum=err2sum+err2[i]

    theta[0] = theta[0] - step_size * err1sum/4  #对应5式
    theta[1] = theta[1] - step_size * err2sum/4  #对应5式

    for i in range (4):
        pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]   #预测值
        error = (1/(2*4))*(pred_y - y[i])**2  #损失值
        loss = loss + error  #总损失值

    iter_count += 1

    print ("iters_count", iter_count)

print ('theta: ',theta )
print ('final loss: ', loss)
print ('iters: ', iter_count)
```

# 牛顿法和逆牛顿法(Newton's method & Quasi-Newton Method)
## 牛顿法
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数$f(x)$的泰勒级数的前几项来寻找方程$f(x) = 0$的根。牛顿法的最大特点就是其收敛速度很快。

从本质上来看，牛顿法是二阶收敛，所以牛顿法会更快。牛顿法在选择方向的时候不仅仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会更大。

牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算复杂。

## 逆牛顿法
拟牛顿法是求解非线性优化问题最有效的方法之一。
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。
